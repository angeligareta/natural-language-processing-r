# Intelligent Systems Course

## Author

- Angel Igareta [angel@igareta.com](angel@igareta.com)

## Packages ----

```{r echo = T, warning = F, results = 'hide'}
# install.packages("rJava")
# install.packages("openNLPmodels.en", repos <- "http://datacube.wu.ac.at", lib = "D:/angel/Documents/R/win-library/3.6")
# install.packages("NLP")
# install.packages("openNLP")
# install.packages("tm")
# install.packages("dplyr")
# install.packages("stringr")
# Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jdk1.8.0_231\\jre') # for 64-bit version

library(rJava)
.jinit(parameters <- "-Xmx4g")
library(NLP)
library(openNLP)
library(openNLPmodels.en)
library(tm)
library(dplyr)
library(stringr)
library(ggplot2)
```

## Utils. [Source: Raúl García-Castro R-Pub](https://rpubs.com/rgcmme/IS-HO3) ----

```{r}
get_annotations_from_document <- function(doc) {
  x <- as.String(doc)
  sent_token_annotator <- Maxent_Sent_Token_Annotator()
  word_token_annotator <- Maxent_Word_Token_Annotator()
  pos_tag_annotator <- Maxent_POS_Tag_Annotator()
  gc()
  y1 <- NLP::annotate(x, list(sent_token_annotator, word_token_annotator))
  y2 <- NLP::annotate(x, pos_tag_annotator, y1)
  parse_annotator <- Parse_Annotator()
  y3 <- NLP::annotate(x, parse_annotator, y2)
  return(y3)
}

get_annotated_merged_document <- function(doc, annotations) {
  x <- as.String(doc)
  y2w <- subset(annotations, type == "word")
  tags <- sapply(y2w$features, '[[', "POS")
  r1 <- sprintf("%s/%s", x[y2w], tags)
  r2 <- paste(r1, collapse = " ")
  return(r2)
}

get_annotated_plain_text_document <- function(doc, annotations) {
  x <- as.String(doc)
  a <- AnnotatedPlainTextDocument(x, annotations)
  return(a)
}

## Returns the pattern detected on an AnnotatedPlainTextDocument.
detect_pattern_on_document <- function(doc, pattern) {
  x <- as.String(doc)
  res <- str_match_all(x, pattern)

  dimrow <- dim(res[[1]])[1]
  dimcol <- dim(res[[1]])[2]

  # If there are no rows, no matches have been found
  if (dimrow == 0) {
    return(NA)
  }else {
    if (dimcol > 2) {
      # If there are three or more columns, we have to paste all the groups together
      for (i in 1:dimrow) {
        res[[1]][i, 2] <- paste(res[[1]][i, 2:dimcol], collapse = ' ')
      }
    }

    # We return all the results found separated by ','
    if (dimcol != 1) {
      result <- paste(res[[1]][, 2], collapse = ', ')
    }else {
      result <- paste(res[[1]][, 1], collapse = ', ')
    }
    return(result)
  }
}

## Returns the pattern detected on an AnnotatedPlainTextDocument with some context.
detect_pattern_on_document_with_context <- function(doc, pattern) {
  txt <- as.String(doc)
  number <- 50
  coord <- str_locate(txt, pattern)
  res3 <- substr(txt, coord[1] - number, coord[2] + number)
  return(res3)
}

## Returns a data frame with all the patterns detected in a corpus.
detect_patterns_in_corpus <- function(corpus, patterns) {
  vall_entities <- data.frame(matrix(NA, ncol = length(patterns) + 1,
                                     nrow = length(corpus)))
  names(vall_entities) <- c("File", patterns)
  for (i in seq_along(patterns)) {
    vall_entities[, i + 1] = unlist(lapply(corpus, detect_pattern_on_document,
                                           pattern = patterns[i]))
  }
  for (i in seq_along(corpus)) {
    vall_entities$File[i] = meta(corpus[[i]])$id
  }
  return(vall_entities)
}


## Returns a data frame with all the patterns detected in an annotated corpus.
detect_patterns_in_tagged_corpus <- function(corpus, taggedCorpus, patterns) {
  vall_entities <- data.frame(matrix(NA, ncol = length(patterns) + 1,
                                     nrow = length(corpus)))
  names(vall_entities) <- c("File", patterns)
  for (i in seq_along(patterns)) {
    vall_entities[, i + 1] = unlist(lapply(taggedCorpus, detect_pattern_on_document,
                                           pattern = patterns[i]))
  }
  for (i in seq_along(corpus)) {
    vall_entities$File[i] = meta(corpus[[i]])$id
  }
  return(vall_entities)
}

## Counts the number of columns with non-NA values for each pattern.
count_matches_per_column <- function(df) {
  entity_count_per_pattern <- data.frame(matrix(NA, ncol = 2,
                                                nrow = length(names(df)) - 1))
  names(entity_count_per_pattern) <- c("Entity", "Count")

  for (i in 2:length(names(df))) {
    entity_count_per_pattern$Entity[i - 1] = names(df)[i]
    entity_count_per_pattern$Count[i - 1] = nrow(subset(df, !is.na(df[i])))
  }
  return(entity_count_per_pattern)
}

## Counts the number of rows with non-NA values for each file.
count_matches_per_row <- function(df) {
  entity_count_per_file <- data.frame(matrix(NA, ncol = 2, nrow = nrow(df)))
  names(entity_count_per_file) <- c("File", "Count")

  for (i in seq_len(nrow(df))) {
    entity_count_per_file$File[i] = df$File[i]
    entity_count_per_file$Count[i] = length(Filter(Negate(is.na), df[i, 2:length(df[i,])]))
  }
  return(entity_count_per_file[entityCountPerFile[2] != 0,])
}

## Prints the matches found per pattern.
print_matches_per_pattern <- function(patterns, matches) {
  for (i in seq_along(patterns)) {
    print(paste("PATTERN: ", patterns[i]))
    strings <- matches[, i + 1][!is.na(unlist(matches[, i + 1]))]
    print(strings)
    print(" ")
  }
}

## Returns a data frame with all the files and their matches in a single list per file.
merge_all_matches_in_lists <- function(df) {
  matches_per_file <- rep(list(list()), nrow(df))
  for (i in seq_len(nrow(df))) {
    matches <- list()
    for (j in 2:ncol(df)) {
      if (grepl(',', df[i, j])) {
        b <- strsplit(as.character(df[i, j]), split = ',')
        for (j in seq_along(b[[1]])) {
          matches <- c(matches, str_trim(b[[1]][j]))
        }
      }else {
        if (!(is.na(df[i, j]))) {
          matches <- c(matches, str_trim(df[i, j]))
        }
      }
    }
    matches <- unique(matches)
    matches_per_file[[i]] <- append(matches_per_file[[i]], matches)
  }

  files <- df[, 1]
  matches <- matches_per_file

  all_matches <- data.frame(matrix(NA, ncol = 2, nrow = nrow(df)))
  names(all_matches) <- c("Files", "Matches")

  all_matches$Files = files
  all_matches$Matches = matches

  return(all_matches)
}

## Returns a data frame with all the files and the gold standard matches in a single list per file.
merge_gold_standard_in_lists <- function(df) {
  matches_per_file <- rep(list(list()), nrow(df))

  for (i in seq_len(nrow(df))) {
    matches <- as.list(unlist(Filter(Negate(is.na), df[i, 2:length(df)])))
    matches_per_file[[i]] <- append(matches_per_file[[i]], matches)
  }

  files <- df[, 1]
  matches <- matches_per_file

  all_matches <- data.frame(matrix(NA, ncol = 2, nrow = nrow(df)))
  names(all_matches) <- c("Files", "Matches")

  all_matches$Files = files
  all_matches$Matches = matches

  return(all_matches)
}

## Calculates precision, recall and f-measure according to a gold standard.
calculate_metrics <- function(matches, matches.gs) {
  metrics <- data.frame(matrix(NA, ncol = 3, nrow = 1))
  names(metrics) <- c("Precision", "Recall", "Fmeasure")

  num_correct <- 0
  all_answers <- 0
  possible_answers <- 0

  for (i in seq_len(nrow(matches))) {
    if (length(matches.gs$Matches[[i]]) != 0) {
      l <- str_trim(unlist(matches[i, 2]))
      l_gs <- unname(unlist(matches.gs[i, 2]))
      intersection <- intersect(l, l_gs)
      num_correct <- num_correct + length(intersection)
      all_answers <- all_answers + length(l)
      possible_answers <- possible_answers + length(l_gs)
    }
  }

  metrics$Precision = num_correct / all_answers
  metrics$Recall = num_correct / possible_answers

  beta <- 1
  if ((metrics$Precision == 0) & (metrics$Recall == 0)) {
    metrics$Fmeasure = 0
  } else {
    metrics$Fmeasure = ((sqrt(beta) + 1) *
      metrics$Precision *
      metrics$Recall) /
      ((sqrt(beta) * metrics$Precision) + metrics$Recall)
  }

  return(metrics)
}
```

## Load corpus ----

```{r}
## Test with a reduced corpus due Java issues
corpus_source <- DirSource("../data/pos-reduced", encoding = "UTF-8")
corpus <- Corpus(corpus_source)
```

## Annotation ----

```{r}
corpus_annotations <- lapply(corpus, get_annotations_from_document)
```

### Show annotations sentences and words

```{r}
head(corpus_annotations[[1]])
tail(corpus_annotations[[1]])
```

### Attach the annotations to the document and store the annotated corpus in another variable

```{r}
corpus_tagged <- Map(get_annotated_plain_text_document, corpus, corpus_annotations)
corpus_tagged[[1]]
```

### Store all the annotations inline with the text and store the annotated corpus in another variable

```{r}
corpus_tagged_text <- Map(get_annotated_merged_document, corpus, corpus_annotations)
corpus_tagged_text[[1]]
```

## Patterns ----

### Given patterns

```{r}
patterns <- "created/VBN by/IN ([A-z]*)/NN ([A-z]*)/NN"
patterns <- c(patterns, "created/VBN by/IN [A-z]*/NN [A-z]*/NN \\(/-LRB- and/CC ([A-z]*)/JJ ([A-z]*)/NN")
patterns <- c(patterns, "screenwriter[s]?/NN[S]? ([A-z]*)/(?:NN[S]?|JJ) ([A-z]*)/(?:NN[S]?|JJ)")
patterns <- c(patterns, "cinematographer/NN(?: ,/,)? ([A-z]*)/(?:NN[S]?|JJ) ([A-z]*)/NN[S]?")
patterns <- c(patterns, "cinematographer/NN(?: ,/,)? ([A-z]*)/NN ([A-z]*)/IN ([A-z]*)/NN")
patterns <- c(patterns, "oscar/NN winner/NN ([A-z]*)/VBG ([A-z]*)/NNS")
```

### Extension patterns

```{r}
patterns <- c(patterns, "screenwriters/NNS [A-z]*/(?:NN[S]?|JJ) [A-z]*/(?:NN[S]?|JJ) \\(/-LRB- [A-z /]* \\)/-RRB- and/CC ([A-z]*)/(?:NN[S]?|JJ) ([A-z]*)/(?:NN|JJ)") # catch second screen writer
patterns <- c(patterns, "([A-z]*)/NN ([A-z]*)/NN is/VBZ [A-z]*/VBG") ## name surnname is verb-ing ....
patterns <- c(patterns, "([A-z]*)/NN ([A-z]*)/NN and/CC [A-z]*/NN[S]?") # name surname and name
patterns <- c(patterns, "([A-z]*)/NN and/CC [A-z]*/JJ [A-z]*/NN[S]?") # name and name surname
```

### Patterns after analyzing the files randomly

```{r}
patterns <- c(patterns, "a/DT ([A-z]*)/(?:NN[S]?|JJ) ([A-z]*)/(?:NN[S]?|JJ) film/NN") # a name surname film
patterns <- c(patterns, "to/TO say/VB ([A-z]*)/NN and/CC [A-z]*/NN") # to say (name) and name
patterns <- c(patterns, "to/TO say/VB [A-z]*/NN and/CC ([A-z]*)/NN") # to say name and (name)
patterns <- c(patterns, "named/VBN ([A-z]*)/(?:NN[S]?|JJ) ([A-z]*)/(?:NN[S]?|JJ)") # named name surname
patterns <- c(patterns, "about/IN ([A-z]*)/(?:NN[S]?|JJ) ([A-z]*)/(?:NN[S]?|JJ)")  # about name surname
patterns <- c(patterns, "made/VBD ([A-z]*)/(?:NN[S]?|JJ) ([A-z]*)/(?:NN[S]?|JJ) a/DT star/NN")  # about name surname
```

### Most significant patterns to increase recall

```{r}
patterns <- c(patterns, "([A-z]*)/(?:NN[S]?|JJ) ([A-z]*)/(?:NN[S]?|JJ|VBP) \\(/-LRB- [A-z]*/(?:NN[S]?|JJ) [A-z]*/(?:NN[S]?|JJ)") # name surname (name surname)
patterns <- c(patterns, "\\(/-LRB- ([A-z]*)/(?:NN[S]?|JJ) ([A-z]*)/(?:NN[S]?|JJ)")  # (name surname)
```

### Patterns that did not work - Generated a lot of FP

```{r}
# patterns <- c(patterns, "[A-z]*/NN and/CC ([A-z]*)/NN") name and name
# patterns <- c(patterns, "([A-z]*)/NN ([A-z]*)/NN is|was/VBZ|VBD [A-z]*/VBG") # name surname is|was verb-ing
```

## Patterns detection ----

```{r}
found_entities <- detect_patterns_in_tagged_corpus(corpus, corpus_tagged_text, patterns)
```

### Print the matches found per pattern.

```{r}
print_matches_per_pattern(patterns, found_entities)
```

## Evaluation ----

### Write results file in a csv

```{r}
write.table(found_entities, file = "results/found_entities.csv", row.names = F, na = "", sep = ";")
```

### Compare with a gold standard

```{r}
matches_list <- merge_all_matches_in_lists(found_entities)
head(found_entities)
```

### Load the gold standard and put all gold standard matches in a list for comparison.

```{r}
gold_standard <- read.table(file = "../data/goldStandard.csv", quote = "", na.strings = "",
                            colClasses = "character", sep = ";")
gold_standard_matches_list <- merge_gold_standard_in_lists(gold_standard)
head(gold_standard_matches_list)
```

### Show lists for first file

```{r}
## MY MATCHES
print(unlist(matches_list$Matches[[2]]))

## GOLD STANDARD MATCHES
print(unlist(gold_standard_matches_list$Matches[[2]]))
```

## Final results

```{r}
metrics <- calculate_metrics(matches_list, gold_standard_matches_list)

## Show metrics
metrics
```

## Conclusion

In this exercise I tried to obtain a higher recall by sacrificing a bit of accuracy, obtaining a great recall of around 0.4 in the files I tested the patterns on and an accuracy of 0.8. The great result is that the F measure is very high, which indicates a good performance.

In order to do so, I did a research in random files to study where the names usually appeared. I found out one of the most common places was between parenthesis, in order to explain a reference. Hence, that is where I found the most significant patterns to increase the recall.

I would have liked to have a more powerful PC to run the metrics over the whole corpus, but I could only select a reduced dataset, as my computer had java memory issues otherwise.
